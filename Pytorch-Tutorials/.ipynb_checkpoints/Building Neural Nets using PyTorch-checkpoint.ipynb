{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a simple 3 layered neural network with 5 nodes in input layer,\n",
    "#3 in hidden layer and 1 in output layer\n",
    "import torch\n",
    "n_input, n_hidden, n_output = 5,3,1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The first step is to do parameter initialization. Here, the weights and bias parameters for each layer are initialized as the tensor variables. Tensors are the base data structures of PyTorch which are used for building different types of neural networks. They can be considered as the generalization of arrays and matrices; in other words, tensors are N-dimensional matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize tensor for inputs and outputs\n",
    "x = torch.randn((1,n_input))\n",
    "y = torch.randn((1,n_output))\n",
    "# intialize tensor variables for weights\n",
    "w1 = torch.randn(n_input,n_hidden) # weight for hidden layer\n",
    "w2 = torch.randn(n_hidden,n_output)# weight for output layer\n",
    "# intialize tensor variables for bias terms\n",
    "b1 = torch.randn((1,n_hidden))\n",
    "b2 = torch.randn((1,n_output))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After the parameter initialization step, a neural network can be defined and trained in four key steps:\n",
    "\n",
    "    Forward Propagation\n",
    "    Loss computation\n",
    "    Backpropagation\n",
    "    Updating the parameters\n",
    "    \n",
    "Forward Propagation: In this step, activations are calculated at every layer using the two steps shown below. These activations flow in the forward direction from the input layer to the output layer in order to generate the final output.\n",
    "\n",
    "    z = weight * input + bias\n",
    "    a = activation_function (z)\n",
    "The following code blocks show how we can write these steps in PyTorch. Notice that most of the functions, such as exponential and matrix multiplication, are similar to the ones in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sigmoid activation function using pytorch\n",
    "def sigmoid_activation(z):\n",
    "    return 1/(1+torch.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## activation for hidden layer\n",
    "z1 = torch.mm(x,w1) +b1\n",
    "a1 = sigmoid_activation(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## activation for output(final) layer\n",
    "z2 = torch.mm(a1,w2) +b2\n",
    "output = sigmoid_activation(z2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Loss Computation: In this step, the error (also called loss) is calculated in the output layer. A simple loss function can tell the difference between the actual value and the predicted value. Later, we will look at different loss functions available in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = y - output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Backpropagation: The aim of this step is to minimize the error in the output layer by making marginal changes in the bias and the weights. These marginal changes are computed using the derivatives of the error term.\n",
    "\n",
    "Based on the Calculus principle of the Chain rule, the delta changes are back passed to hidden layers where corresponding changes in their weights and bias are made. This leads to an adjustment in the weights and bias until the error is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate the derivative of activation\n",
    "def sigmoid_delta(x):\n",
    "  return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute derivative of error terms\n",
    "delta_output = sigmoid_delta(output)\n",
    "delta_hidden = sigmoid_delta(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backpass the changes to previous layers\n",
    "d_outp = loss* delta_output\n",
    "loss_h   = torch.mm(d_output,w2.t())\n",
    "d_hidn   = loss_h * delta_hidden"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Updating the Parameters: Finally, the weights and bias are updated using the delta changes received from the above backpropagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 += torch.mm(a1.t(), d_outp) * learning_rate\n",
    "w1 += torch.mm(x.t(), d_hidn) * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 += d_outp.sum() * learning_rate\n",
    "b1 += d_hidn.sum() * learning_rate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, when these steps are executed for a number of epochs with a large number of training examples, the loss is reduced to a minimum value. The final weight and bias values are obtained which can then be used to make predictions on the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
